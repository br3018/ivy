# Ivy AI Orchestration System - Environment Configuration
# Copy this file to .env and configure with your actual values

# ==============================================================================
# LLM Provider Configuration
# ==============================================================================
# Choose LLM provider: "local" for Ollama or "claude" for Anthropic Claude
LLM_PROVIDER=local

# Ollama Configuration (for local LLM)
OLLAMA_MODEL=llama3.1
# Available models: llama3.1, llama3.2, mistral, codellama, etc.
# Install with: docker exec ollama ollama pull <model-name>

# Claude Configuration (for Anthropic API)
ANTHROPIC_API_KEY=your-anthropic-api-key-here
# Get your API key from: https://console.anthropic.com/settings/keys
# Leave empty if using local LLM only

# ==============================================================================
# Service URLs (Docker Compose default - usually don't need to change)
# ==============================================================================
OLLAMA_BASE_URL=http://ollama:11434
QDRANT_URL=http://vector-db:6333
KNOWLEDGE_AGENT_URL=http://knowledge:8001

# ==============================================================================
# Model Cache Configuration
# ==============================================================================
HF_HOME=/app/.cache/huggingface
# Hugging Face cache directory for ColQwen2 models
